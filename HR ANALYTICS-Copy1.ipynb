{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# JOB CHANGE OF DATA SCIENTISTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-7ac0876c2352>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# Ensemble learning models\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensemble\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBaggingClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBaggingRegressor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGradientBoostingRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBClassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;31m# Pipeline and column transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Pipeline and column transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "# Data transformers\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "\n",
    "\n",
    "# Data splitter and model evaluator\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# Learning models (use one of them or any other model)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Ensemble learning models\n",
    "from sklearn.ensemble import BaggingClassifier, BaggingRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
    "from xgboost import XGBClassifier, XGBRegressor \n",
    "\n",
    "# Pipeline and column transformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.compose import make_column_transformer\n",
    "\n",
    "# Performance metrics\n",
    "from sklearn.metrics import log_loss, roc_auc_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('aug_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('aug_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lets print full summary of dataframe\n",
    "df_train.info()\n",
    "# we see dataframe has 19,157 rows, 14 columns, data type in each column, and number of non-null values in each column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have separate training and test data sets. lets combine train and test together to do common feature engineering\n",
    "train_replica = df_train.copy()\n",
    "test_replica = df_test.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a flag field to distinguish records from training and testing sets in the combined dataset\n",
    "train_replica['tst'] = 0\n",
    "test_replica['tst'] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine training and testing data into a single dataframe to do uniform part of feature engineering\n",
    "combined_data = pd.concat([train_replica, test_replica], axis=0, sort=True)\n",
    "del train_replica\n",
    "del test_replica"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import nan\n",
    "from numpy import isnan\n",
    "values=combined_data.values\n",
    "imputer= SimpleImputer(missing_values=nan, strategy='most_frequent')\n",
    "transformed_values=imputer.fit_transform(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Categorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Nominal: gender, enrolled_university, major_discipline, company_type,   \n",
    "#Ordinal: company_size, education_level, last_new_job, relevent_experience\n",
    "#Numerical: experience, training_hours"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lets begin by handling NaN missing values in ALL columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mode_values=combined_data [['gender', 'enrolled_university', 'major_discipline', 'company_type']].mode()\n",
    "print(mode_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lets fill all NaN values in ALL COLUMNS(imputing)\n",
    "imputer = SimpleImputer(missing_values = np.nan,  \n",
    "                        strategy ='most_frequent') \n",
    "combined_data.gender=imputer.fit_transform(combined_data['gender']. values.reshape(-1,1))[:,0]\n",
    "combined_data.relevent_experience=imputer.fit_transform(combined_data['relevent_experience']. values.reshape(-1,1))[:,0]\n",
    "combined_data.education_level=imputer.fit_transform(combined_data['education_level']. values.reshape(-1,1))[:,0]\n",
    "combined_data.major_discipline=imputer.fit_transform(combined_data['major_discipline']. values.reshape(-1,1))[:,0]\n",
    "combined_data.experience=imputer.fit_transform(combined_data['experience']. values.reshape(-1,1))[:,0]\n",
    "combined_data.company_size=imputer.fit_transform(combined_data['company_size']. values.reshape(-1,1))[:,0]\n",
    "combined_data.company_type=imputer.fit_transform(combined_data['company_type']. values.reshape(-1,1))[:,0]\n",
    "combined_data.last_new_job=imputer.fit_transform(combined_data['last_new_job']. values.reshape(-1,1))[:,0]\n",
    "combined_data.training_hours=imputer.fit_transform(combined_data['training_hours']. values.reshape(-1,1))[:,0]\n",
    "combined_data.enrolled_university=imputer.fit_transform(combined_data['enrolled_university']. values.reshape(-1,1))[:,0]\n",
    "combined_data.target=imputer.fit_transform(combined_data['target']. values.reshape(-1,1))[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_data.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking list of all values in columns so we can code them "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_data.gender.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.relevent_experience.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.enrolled_university.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.major_discipline.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.company_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.last_new_job.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One Hot Encoding of Nominal Variables (company_type, enrolled_university, gender, major_discipline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to use label encoder before we use on-hot encoding. Label endocer will convert string values to numerical values.\n",
    "# we need to encode every categorical feature separately, meaning we need as many encoders as categorical features. \n",
    "#Letâ€™s loop over all categorical features and build a dictionary that will map a feature to its encoder:\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "# For each categorical column\n",
    "# We fit a label encoder, transform our column and add it to our new dataframe\n",
    "\n",
    "nom_columns = [\"gender\", \"enrolled_university\", \"company_type\", \"major_discipline\"]\n",
    "label_encoders = {}\n",
    "for col in nom_columns:\n",
    "    print(\"Encoding {}\".format(col))\n",
    "    new_le = LabelEncoder()\n",
    "    combined_data[col] = new_le.fit_transform(combined_data[col])\n",
    "    label_encoders[col] = new_le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Now that we have label encoded nominal features; we can do one hot encoding. \n",
    "ohe = OneHotEncoder(handle_unknown='ignore')\n",
    "ohe.fit_transform( combined_data[['gender','enrolled_university','company_type','major_discipline']] ).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.drop(['city', 'city_development_index', 'enrollee_id'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mapping Ordinal Features \n",
    "# (company_size, education_level, last_new_job, relevent_experience)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordinal features are company_size, education_level, last_new_job, relevent_experience\n",
    "# Lets find unique values in each of these columns so we can map them.\n",
    "# Oridnal featues are mapped; one-hot-encoding is not done on oridnal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "combined_data.relevent_experience.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_data.education_level.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "combined_data.company_size.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_data.last_new_job.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Mapping ordinal features. Creating new columns for ordinal features. \n",
    "relevent_experience_map= {'Has relevent experience': 1, 'No relevent experience':2}\n",
    "combined_data['relevent_experience_ordinal'] = combined_data.relevent_experience.map(relevent_experience_map)\n",
    "\n",
    "education_level_map = {'Primary School': 1, 'High School':2, 'Masters':3, 'Graduate':4, 'Phd':5}\n",
    "combined_data['education_level_ordinal'] = combined_data.education_level.map(education_level_map)\n",
    "\n",
    "company_size_map = {'<10': 1, '10/49':1, '50-99':2, '100-500':3, '500-999':4, '1000-4999':5, '5000-9999':6, '10000+':7}\n",
    "combined_data['company_size_ordinal'] = combined_data.company_size.map(company_size_map)\n",
    "\n",
    "last_new_job_map = {'never': 0, '1':1, '2':2, '3':3, '4':4, '>4':5}\n",
    "combined_data['last_new_job_ordinal'] = combined_data.last_new_job.map(last_new_job_map)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleaning columns with continuous data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In column experience managing the < and > signs\n",
    "combined_data['experience'] = combined_data['experience'].replace(['>20', '<1'], ['20', '1'])\n",
    "combined_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining predictor and traget variables. Splitting data into test and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define Predictors and Target Variable\n",
    "nom_col = ['gender','enrolled_university', 'major_discipline', 'company_type'] \n",
    "ord_col = ['company_size_ordinal', 'education_level_ordinal', 'last_new_job_ordinal', 'relevent_experience_ordinal']\n",
    "num_col = ['experience', 'training_hours']   \n",
    "\n",
    "X= combined_data[nom_col + ord_col + num_col]\n",
    "y= combined_data['target']\n",
    "\n",
    "# Splitting Data into Training Set and Test Set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    test_size=0.3, \n",
    "                                                    stratify=y, \n",
    "                                                    random_state=4\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking for balanced or imbalanced data (since its a classification prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(combined_data['target'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset is imbalanced. For imbalanced dataset performance metrics will be Confusion Matrix, \n",
    "Precicion, Recall, and F1 Score. We use Accuracy only when dataset is balanced (50/50 or 60/40 difference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Recall = TP/(TP+FN)      \n",
    "Out of total actual positive values how many positive did we predict correctly (TPR or Sensitivity)\n",
    "When you want to reduce FN (eg cancer or not) use Recall.\n",
    "\n",
    "Precision = TP/ (TP+FP)  \n",
    "Out of total actual predicted positive rsults how many results were actual positive (Positive Prediction Value)  \n",
    "When your FP is important (you want to reduce it, eg spam or not) value use Precision\n",
    "\n",
    "If FP and FN are noth important then use weighted average of FP and Fn which is called F1 SCORE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardizing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making pipeline branches. Creating the main pipe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In the following block we experiment with different models. \n",
    "# The one with closely matching training accuracy and test accuracy will be used for hyperparameter tuning.\n",
    "# If training accuracy is much higher than validation/test accuracy that means model suffers from HIGH VARIANCE. \n",
    "# HIGH VARAINCE= OVERFITTING = model fits training data perfectly but does not do a good job with out of model data\n",
    "# If test accuracy is signifcantly higher than training accuracy mlodel suffers from HIGH BIAS.\n",
    "# HIGH BIAS= UNDERFITTING = decision boundary is very simple thus fails to capture important relations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Branch for nominal features\n",
    "nom_pipe = make_pipeline(SimpleImputer(strategy='most_frequent'),\n",
    "                         OneHotEncoder(handle_unknown='ignore')\n",
    "                        )\n",
    "# Branch for ordinal features\n",
    "ord_pipe = make_pipeline(SimpleImputer(strategy='median'),\n",
    "                         StandardScaler()\n",
    "                        )\n",
    "# Branch for numerical features\n",
    "num_pipe = make_pipeline(SimpleImputer(strategy='mean'),\n",
    "                         MinMaxScaler()\n",
    "                        )\n",
    "# Make the main pipe, in which a column transformer sends columns into relevent pipes\n",
    "pipe = make_pipeline(ColumnTransformer( [ ('nom', nom_pipe, nom_col),\n",
    "                                          ('ord', ord_pipe, ord_col),\n",
    "                                          ('num', num_pipe, num_col) ] ),\n",
    "                     #PCA(n_components=3),\n",
    "                     #LDA(n_components=8),\n",
    "                     #SVC(kernel='rbf', C=1000, gamma=1)\n",
    "                     #LogisticRegression(solver='lbfgs', C=0.01)\n",
    "                     DecisionTreeClassifier(criterion='gini', max_depth=3)\n",
    "                     #RandomForestClassifier(criterion='gini', n_estimators=20, random_state=1)\n",
    "                     #KNeighborsClassifier(n_neighbors=10, p=2)\n",
    "                    )\n",
    "pipe.fit(X_train,y_train)\n",
    "\n",
    "print('Training score:', pipe.score(X_train,y_train))\n",
    "print(\"Test accuracy: \", pipe.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Model Performance : Confusion Matrix & ROC AUC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_perfomance_details(y_pred,y_test):\n",
    "    #Classification report\n",
    "    print (\"Classification Report:\\n\")\n",
    "    print (classification_report(y_true=y_test,y_pred=y_pred))\n",
    "    \n",
    " #Draw confusion matrix\n",
    "sns.reset_defaults()\n",
    "plt.figure(figsize=(7,4));\n",
    "sns.heatmap(confusion_matrix(y_test,y_pred),annot=True,cmap=\"GnBu\",fmt=\"g\",cbar=False);\n",
    "plt.title(\"Confusion Matrix\");\n",
    "plt.show()\n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " #Find the Area under the curve\n",
    "print (\"\\n----------------\\nAOC ROC details\\n----------------\\n\")\n",
    "rocauc_score=roc_auc_score(y_pred,y_test)\n",
    "    \n",
    "#ROC curve\n",
    "fpr,tpr,_=roc_curve(y_test,y_pred)\n",
    "roc_aoc=auc(fpr,tpr)\n",
    "print (f\"AUC score: {rocauc_score}\\nTrue positive rate: {tpr}\\nFalse postive rate: {fpr}\")\n",
    "\n",
    "\n",
    "#Draw the ROC curve\n",
    "plt.figure(figsize=(4,4));\n",
    "lw=2\n",
    "plt.plot(fpr,tpr,\n",
    "            color='green',\n",
    "            lw=lw,\n",
    "            label='ROC curve (area=%0.4f)' % roc_aoc);\n",
    "    \n",
    "#plot diagonal line  from (0,0) to (1,1), represents fpt=tpr\n",
    "plt.plot([0,1],[0,1],color='lightgrey',lw=lw,linestyle='--');\n",
    "plt.xlim([0.0,1.0]);\n",
    "plt.ylim([0.0,1.0]);\n",
    "plt.xlabel(\"False Positive Rate\");\n",
    "plt.ylabel(\"True Postive Rate\");\n",
    "plt.title(\"Reciever operating characteristic for training data\");\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning Curve: to see if model perfirmance can get better by collecting more samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance Metrics for the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\nMeasuring performance using log loss (the lower the better):')\n",
    "print('Training set log loss:', log_loss(y_train, pipe.predict_proba(X_train)))\n",
    "print('Test set     log loss:', log_loss(y_test,  pipe.predict_proba(X_test)))\n",
    "\n",
    "print('\\nMeasuring performance using roc_auc  (the higher the better: highest possible is 1, random guess is 0.5):')\n",
    "print('Training set roc auc:', roc_auc_score(y_train, pipe.predict_proba(X_train)[:,1]))\n",
    "print('Test set     roc auc:', roc_auc_score(y_test,  pipe.predict_proba(X_test) [:,1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning & Validation Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A learning curve plots the score over varying numbers of training samples, while a validation curve plots the score over \n",
    "# a varying hyper parameter. The learning curve is a tool for finding out if an estimator would benefit from more data, \n",
    "#or if the model is too simple (biased). If the training curve and validation curves converge that means this classifier \n",
    "# would hardly benefit from adding more training data; a more expressive model may be more appropriate.\n",
    "\n",
    "# The validation curve is a tool for finding good hyper parameter settings. \n",
    "# Some hyper parameters (number of neurons in a neural network, maximum tree depth in a decision tree, \n",
    "# amount of regularization, etc.) control the complexity of a model. \n",
    "# We want the model to be complex enough to capture relevant information in the training data but not too complex to avoid \n",
    "# overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#param_name  = 'svc__gamma'\n",
    "#param_range = np.logspace(-5, 1, 13)\n",
    "#param_name  = 'logisticregression__C'\n",
    "#param_range = np.logspace(-4, 2, 13)\n",
    "param_name  = 'decisiontreeclassifier__max_depth'\n",
    "param_range = np.arange(1,15)\n",
    "#param_name  = 'randomforestclassifier__max_depth'\n",
    "#param_range = np.arange(1,40)\n",
    "#param_name  = 'kneighborsclassifier__n_neighbors'\n",
    "#param_range = np.arange(1,26,2)\n",
    "\n",
    "\n",
    "#scoring='r2'           # for regression problems\n",
    "#scoring='accuracy'     # for classification problems with balanced target varaiable values\n",
    "#scoring='neg_log_loss' # for classification problems\n",
    "scoring='roc_auc'      # for classification problems\n",
    "\n",
    "\n",
    "\n",
    "train_scores, val_scores = validation_curve(estimator=pipe, X=X_train, y=y_train, \n",
    "                                            cv=10,\n",
    "                                            param_name=param_name, \n",
    "                                            param_range=param_range,\n",
    "                                            scoring = scoring\n",
    "                                            )\n",
    "\n",
    "trn_mean = np.mean(train_scores, axis=1)\n",
    "trn_std  = np.std (train_scores, axis=1)\n",
    "val_mean = np.mean(val_scores, axis=1)\n",
    "val_std  = np.std (val_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(param_range, trn_mean, 'bo-',  markersize=5, label='training accuracy')\n",
    "plt.fill_between(param_range, trn_mean+trn_std, trn_mean-trn_std, alpha=0.25, color='blue')\n",
    "\n",
    "plt.plot(param_range, val_mean, 'gs--', markersize=5, label='validation accuracy')\n",
    "plt.fill_between(param_range, val_mean+val_std, val_mean-val_std, alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "#plt.xscale('log')  # Use this only when param_range = np.logspace(...). Comment this out otherwise.\n",
    "plt.legend(loc='upper center', fontsize=14)\n",
    "plt.xlabel(param_name, fontsize=14)\n",
    "plt.ylabel(scoring, fontsize=14)\n",
    "#plt.savefig('val_curve')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning Curve: to see if model can benefit from collecting more samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(estimator=pipe, X=X_train, y=y_train,\n",
    "                                                        train_sizes=np.linspace(0.03, 1.0, 5),\n",
    "                                                        cv=5,\n",
    "                                                        scoring=scoring\n",
    "                                                       )\n",
    "train_mean= np.mean(train_scores, axis=1)\n",
    "train_std = np.std (train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "test_std  = np.std (test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(train_sizes, train_mean, 'bo-', markersize=5, label='training '+ scoring)\n",
    "plt.fill_between(train_sizes, train_mean + train_std, train_mean - train_std, alpha=0.25, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, test_mean, 'gs--', markersize=5, label='validation '+ scoring)\n",
    "plt.fill_between(train_sizes, test_mean + test_std,  test_mean - test_std,   alpha=0.15, color='green')\n",
    "\n",
    "plt.grid()\n",
    "plt.xlabel('Number of training samples', fontsize=14)\n",
    "plt.ylabel(scoring, fontsize=14)\n",
    "plt.legend(loc='best', fontsize=14)\n",
    "# plt.savefig('learning_curve', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Confusion Matrix "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is an unbalanced data set. \n",
    "# We will use confucion matrix to see TP and TN. We will aim to reduce FP and FN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=2)\n",
    "display_labels= [0,1]\n",
    "\n",
    "# Plot non-normalized confusion matrix\n",
    "titles_options = [(\"Confusion matrix, without normalization\", None),\n",
    "                  (\"Normalized confusion matrix\", 'true')]\n",
    "for title, normalize in titles_options:\n",
    "    disp = confusion_matrix(pipe, X_test, y_test,\n",
    "                            display_labels= [0,1],\n",
    "                            cmap=plt.cm.Blues,\n",
    "                            normalize=normalize)\n",
    "    disp.ax_.set_title(title)\n",
    "\n",
    "    print(title)\n",
    "    print(disp.confusion_matrix)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_score(y_train, y_train_pred)\n",
    "recall_score(y_train_5, y_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
